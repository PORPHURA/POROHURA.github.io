<!DOCTYPE html>
<html>
  <head>
    <title>Luzhe Huang's Personal Page</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
    html,body,h1,h2,h3,h4,h5,h6 {font-family: "Roboto", sans-serif}
    </style>
  </head>
<body class="w3-light-grey">

<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:1400px;">

  <!-- The Grid -->
  <div class="w3-row-padding">
  
    <!-- Left Column -->
    <div class="w3-third">
    
      <div class="w3-white w3-text-grey w3-card-4">
        <div class="w3-display-container">
          <img src="Portrait.jpeg" style="width:100%" alt="Avatar">
          <div class="w3-display-bottomleft w3-container w3-text-white">
            <h2>Luzhe Huang</h2>
          </div>
        </div>
        <div class="w3-container">
          <p><i class="fa fa-briefcase fa-fw w3-margin-right w3-large w3-text-teal"></i>PhD Candidate</p>
          <p><i class="fa fa-home fa-fw w3-margin-right w3-large w3-text-teal"></i>Los Angeles, USA</p>
          <p><i class="fa fa-envelope fa-fw w3-margin-right w3-large w3-text-teal"></i>lzhuang0324@ucla.edu</p>
          <p><i class="fa fa-phone fa-fw w3-margin-right w3-large w3-text-teal"></i>+(1)424-402-2604</p>
          <p><i class="fa fa-brand fa-google fa-fw w3-margin-right w3-large w3-text-teal"></i><a href="https://scholar.google.com/citations?user=qq9rtkkAAAAJ&hl=en&oi=ao">Google Scholar Page</a></p>
          <p><i class="fa fa-brand fa-linkedin fa-fw w3-margin-right w3-large w3-text-teal"></i><a href="https://www.linkedin.com/in/璐哲-黄-59854b155/">LinkedIn Page</a></p>
          <p><i class="fa fa-brand fa-github fa-fw w3-margin-right w3-large w3-text-teal"></i><a href="https://github.com/PORPHURA">GitHub Page</a></p>
          <p><i class="fa fa-solid fa-user fa-fw w3-margin-right w3-large w3-text-teal"></i><a href='Luzhe_Huang_CV.pdf'>CV</a>
        </div>
      </div><br>

    <!-- End Left Column -->
    </div>

    <!-- Right Column -->
    <div class="w3-twothird">
      
      <div class="w3-container w3-card w3-white">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-user fa-fw w3-margin-right w3-xxlarge w3-text-teal"></i>Biography</h2>
        <div class="w3-container">
          <p>I am a PhD Candidate at UCLA Bio- and Nano-photonics Lab, where I am supervised by <a href="https://www.ee.ucla.edu/aydogan-ozcan/">Professor Aydogan Ozcan</a>. I received Master of Science in June 2021 at UCLA. I received Bachelor of Engineering (BEng) degree in June 2019 at Zhejiang University, China.</p>
          <p>My current research lie in computational microscopy. My goal is to apply machine and deep learning technologies to microscopy imaging, and also enhance the robustness, generalizability and interpretability of models. </p>
        </div>

      </div>
      <br>

      <div class="w3-container w3-card w3-white">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-certificate fa-fw w3-margin-right w3-xxlarge w3-text-teal"></i>News</h2>
        <div class="w3-container">
          <h6 class="w3-text-teal"><i class="fa fa-calendar fa-fw w3-margin-right"></i>06/2021</h6>
          <p>One co-first author paper is published on ACS Photonics.</p>
          <hr>
        </div>
        <div class="w3-container">
          <h6 class="w3-text-teal"><i class="fa fa-calendar fa-fw w3-margin-right"></i>06/2021</h6>
          <p>I received my Master Degree at UCLA.</p>
          <hr>
        </div>

      </div>
      <br>

      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-suitcase fa-fw w3-margin-right w3-xxlarge w3-text-teal"></i>Selected Publications</h2>
        <div class="w3-twothird w3-container">
          <h5 class="w3-opacity"><b>Recurrent neural network-based volumetric fluorescence microscopy</b></h5>
          <h6 class="w3-text-teal"><em>Light: Science and Applications</em> (IF=17.782)</h6>
          <p>Volumetric imaging of samples using fluorescence microscopy plays an important role in various fields including physical, medical and life sciences. In this work we report a deep learning-based volumetric image inference framework that uses 2D images that are sparsely captured by a standard wide-field fluorescence microscope at arbitrary axial positions within the sample volume. Our work demonstrates the first application of recurrent neural networks in microscopic image reconstruction and provides a flexible and rapid volumetric imaging framework.</p>
          <hr>
        </div>
        <div class="w3-third w3-container">
          <a href="https://www.nature.com/articles/s41377-021-00506-9"><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41377-021-00506-9/MediaObjects/41377_2021_506_Fig1_HTML.png?as=webp" style="width:100%"></a>
        </div>
        <div class="w3-twothird w3-container">
          <h5 class="w3-opacity"><b>Single-shot autofocusing of microscopy images using deep learning</b></h5>
          <h6 class="w3-text-teal"><em>ACS Photonics</em> (IF=7.529)</h6>
          <p>Autofocusing is a critical step for high-quality microscopic imaging of specimens, especially for measurements that extend over time covering large fields of view. In this work, we demonstrate a deep learning-based offline autofocusing method, termed Deep-R, that is trained to rapidly and blindly autofocus a single-shot microscopy image of a specimen that is acquired at an arbitrary out-of-focus plane. Our results reveal that Deep-R is significantly faster when compared with standard online algorithmic autofocusing methods, also reducing the photon dose on the sample.</p>
          <hr>
        </div>
        <div class="w3-third w3-container">
          <a href="https://pubs.acs.org/doi/abs/10.1021/acsphotonics.0c01774"><img src="DeepR_cover.jpg" style="width:100%"></a>
        </div>
        <div class="w3-twothird w3-container">
          <h5 class="w3-opacity"><b>SNR-Adaptive OCT Angiography Enabled by Statistical Characterization of Intensity and Decorrelation With Multi-Variate Time Series Model</b></h5>
          <h6 class="w3-text-teal"><em>IEEE Trans. Med. Imaging</em> (IF=10.048)</h6>
          <p>In OCT angiography (OCTA), decorrelation computation has been widely used as a local motion index to identify dynamic flow from static tissues, but its dependence on SNR severely degrades the vascular visibility, particularly in low-SNR regions. We developed a multi-variate time series (MVTS) model, and based on this model, derived a universal asymptotic linear relation of decorrelation to inverse SNR (iSNR) and a linear classifier, termed as ID-OCTA.</p><br>
        </div>
        <div class="w3-third w3-container">
          <a href="https://ieeexplore.ieee.org/abstract/document/8689077"><img src="OCTA.jpg" style="width:100%"></a>
        </div>
      </div>


    <!-- End Right Column -->
    </div>
    
  <!-- End Grid -->
  </div>
  
  <!-- End Page Container -->
</div>

<!-- Footer. This section contains an ad for W3Schools Spaces. You can leave it to support us. -->
<footer class="w3-container w3-teal w3-center w3-margin-top">
  <p>Copyright © Luzhe Huang 2022.</p>
  <p>Last updated at 05/2022.</p>
 </footer>

</body>
</html>
